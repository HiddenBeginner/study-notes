# Conservative Q-Learning (CQL)

- 제목: Conservative Q-Learning for Offline Reinforcement Learning
- 저자: Kumar, Aviral, Aurick Zhou, George Tucker, Sergey Levine, *UC Berkeley*
- 연도: 2020년
- 학술대회: NeurIPS
- 링크: [https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html)
- 키워드: Offline RL

---

## 풀고자 하는 문제

### Offline RL과 distribution shift

환경과 상호작용하며 에이전트를 학습시키는 기존 강화학습과 다르게, offline RL은 환경과 상호작용할 수 없으며, 미리 수집된 transitions 데이터셋을 사용하여 에이전트를 학습시킨다.
미리 수집된 데이터셋 $\mathcal{D}$는 어떤 행동 정책 $\pi_\beta$에 의해 만들어졌다고 가정하며, 최소 그 행동 정책과 비슷한 수준의 정책 또는 더 좋은 정책 $\pi$를 찾는 것을 목표로 한다.

이때, 
- 데이터셋에 있는 state-action pair $(\mathbf{s},\mathbf{a})\in\mathcal{D}$의 empirical 분포와 
- 학습된 정책 $\pi$가 만드는 state-action pair $(\mathbf{s}, \mathbf{a})$ where $\mathbf{s} \in \mathcal{D}$ and $\mathbf{a} \sim \pi(\cdot|\mathbf{s})$의 분포가

다를 수 있으며, 이를 distributional shift라고 부른다. Online RL의 경우 학습된 정책으로 직접 환경과 상호작용하여 이전에 보지 못한 state-action pair을 경험해볼 수 있지만, offline RL의 경우 환경과 상호작용할 수 없기 때문에 학습된 정책으로 만든 state-action pair에 에러가 있을 경우 이를 correction할 수 없다.

<br>

### Distribution shift의 문제점 알아보기

Distribution shift가 Q-learning에 미치는 영향을 알아보기 위해 먼저 Bellman operator를 떠올려보자.

$$
\mathcal{B}Q^{\pi}(\mathbf{s, \mathbf{a}}) = r(\mathbf{s, \mathbf{a}}) + \gamma \mathbb{E}_{s' \sim p(\cdot|\mathbf{s}, \mathbf{a}), a' \in \pi(\cdot|\mathbf{s'})}\left[ Q^\pi(\mathbf{s'}, \mathbf{a'})\right].
$$

<br>

일반적으로 reward function $r$과 transition probability distribution $p$를 알 수 없기 때문에, buffer $\mathcal{D}$에 있는 transition $(\mathbf{s}, \mathbf{a}, r, \mathbf{s}')$을 사용하여 행동가치함수를 학습한다. Empirical Bellman operator $\hat{\mathcal{B}}$는 다음과 같다.

$$
\hat{\mathcal{B}}Q^{\pi}(\mathbf{s, \mathbf{a}}) = r + \gamma \mathbb{E}_{\mathbf{a}' \sim \hat{\pi}(\cdot | \mathbf{s}')}\left[ Q(\mathbf{s}', \mathbf{a}')\right] \; \text{ for } (\mathbf{s}, \mathbf{a}, r, \mathbf{s}') \in \mathcal{D}.
$$

<br>

보상 $r$과 다음 상태 $\mathbf{s}'$이 환경 모델로부터 샘플링되는 것이 아니라 그냥 데이터를 대입하는 것이다. Empirical Bellman operator를 바탕으로 TD 에러를 최소화하여 policy evaluation을 하는 것을 식으로 적어보면 다음과 같다.

$$
\hat{Q}^{k+1} \leftarrow \operatorname*{argmin}_{Q}\mathbb{E}_{(\mathbf{s}, \mathbf{a}, r, \mathbf{s}') \in \mathcal{D}} \left[ \left( \left( r +\gamma\mathbb{E}_{\mathbf{a}' \sim \hat{\pi}^k(\cdot | \mathbf{s}')}\left[ \hat{Q}^k(\mathbf{s}', \mathbf{a}')\right]\right) - Q(\mathbf{s}, \mathbf{a}) \right)^2\right].
$$

<br>

위 식에서 $(\mathbf{s}, \mathbf{a}, r, \mathbf{s}')$은 우리가 갖고 있는 데이터이다. 
- 문제가 되는 부분은 정책에서 샘플링한 다음 행동 $\mathbf{a}'$에 대해 계산되어야 하는 $\hat{Q}^k(\mathbf{s}', \mathbf{a}')$이다. 
- 더 문제가 되는 경우는 우리의 데이터셋 $\mathcal{D}$에 $(\mathbf{s}', \mathbf{a}')$에 대한 transition이 전혀 없는 경우이다. 이런 행동을 out-of-distribution action이라고 부른다. 
- 이보다 더 문제가 되는 경우는 $\hat{Q}^k(\mathbf{s}', \mathbf{a}')$이 overestimation 되어 있는 경우이다. Offline RL의 경우 ood action에 대한 행동가치함수 $\hat{Q}^k(\mathbf{s}', \mathbf{a}')$는 타겟으로만 사용되고 직접 업데이트되지 않기 때문에 overestimation이 correction 될 수 없다.  

<br>

이 논문에서는 Conservative Q-Learning (CQL)이라는 알고리즘을 제시하여 overestimation을 근절시킨다. Conservative Q-learning으로 학습한 행동가치함수는 실제 행동가치함수의 lower bound가 된다.

<br>

---

## Conservative Q-Learning

Coming soon! 

---

## Reference

[1] Kumar, Aviral, Aurick Zhou, George Tucker, and Sergey Levine. “Conservative Q-Learning for Offline Reinforcement Learning.” In Advances in Neural Information Processing Systems, 33:1179–91. Curran Associates, Inc., 2020. https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html.
