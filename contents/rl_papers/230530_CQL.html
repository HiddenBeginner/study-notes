
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Conservative Q-Learning (CQL) &#8212; 공부 기록</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Define-and-run vs Define-by-run" href="../miscellaneous/define_and_by_run.html" />
    <link rel="prev" title="Dreamer" href="DreamerV1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">공부 기록</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    공부 기록
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ML/DL 코어
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../core/beyond_batchnorm.html">
   Beyond BatchNorm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../core/230420_ulc.html">
   Uncertainty-aware Label Correction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../core/230425_encouraging_loss.html">
   Encouraging Loss
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../core/230523_folklore_constructions.html">
   Elementary Folklore Construction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  강화학습
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="AlphaGoZero.html">
   AlphaGo Zero
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="SUNRISE.html">
   SUNRISE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DreamerV1.html">
   Dreamer
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Conservative Q-Learning (CQL)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  기타
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../miscellaneous/define_and_by_run.html">
   Define-and-run vs Define-by-run
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../miscellaneous/wasserstein_distance.html">
   Wasserstein distance 구현하기
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../miscellaneous/quantile_regression.html">
   Quantile Regression
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  튜토리얼
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/2023-04-20_CartRacing-v2_DQN.html">
   Control CartRacing-v2 environment using DQN from scratch
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcontents/rl_papers/230530_CQL.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/contents/rl_papers/230530_CQL.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   1. 풀고자 하는 문제
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#offline-rl-distribution-shift">
     1.1 Offline RL과 distribution shift
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distribution-shift">
     1.2 Distribution shift의 문제점 알아보기
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conservative-q-learning">
   2. Conservative Q-Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conservative-off-policy-evaluation">
     2.1 Conservative Off-Policy Evaluation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conservative-q-learning-for-offline-rl">
     2.2 Conservative Q-Learning for Offline RL
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experimental-evaluation">
   Experimental Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   마무리하며
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reference">
   Reference
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Conservative Q-Learning (CQL)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   1. 풀고자 하는 문제
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#offline-rl-distribution-shift">
     1.1 Offline RL과 distribution shift
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distribution-shift">
     1.2 Distribution shift의 문제점 알아보기
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conservative-q-learning">
   2. Conservative Q-Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conservative-off-policy-evaluation">
     2.1 Conservative Off-Policy Evaluation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conservative-q-learning-for-offline-rl">
     2.2 Conservative Q-Learning for Offline RL
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experimental-evaluation">
   Experimental Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   마무리하며
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reference">
   Reference
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="conservative-q-learning-cql">
<h1>Conservative Q-Learning (CQL)<a class="headerlink" href="#conservative-q-learning-cql" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>제목: Conservative Q-Learning for Offline Reinforcement Learning</p></li>
<li><p>저자: Kumar, Aviral, Aurick Zhou, George Tucker, Sergey Levine, <em>UC Berkeley</em></p></li>
<li><p>연도: 2020년</p></li>
<li><p>학술대회: NeurIPS</p></li>
<li><p>링크: <a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html</a></p></li>
<li><p>키워드: Offline RL</p></li>
</ul>
<hr class="docutils" />
<section id="id1">
<h2>1. 풀고자 하는 문제<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<section id="offline-rl-distribution-shift">
<h3>1.1 Offline RL과 distribution shift<a class="headerlink" href="#offline-rl-distribution-shift" title="Permalink to this headline">#</a></h3>
<p>환경과 상호작용하며 에이전트를 학습시키는 기존 강화학습과 다르게, offline RL은 환경과 상호작용할 수 없으며, 미리 수집된 transitions 데이터셋을 사용하여 에이전트를 학습시킨다.
미리 수집된 데이터셋 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>는 어떤 행동 정책 <span class="math notranslate nohighlight">\(\pi_\beta\)</span>에 의해 만들어졌다고 가정하며, 최소 그 행동 정책과 비슷한 수준의 정책 또는 더 좋은 정책 <span class="math notranslate nohighlight">\(\pi\)</span>를 찾는 것을 목표로 한다.</p>
<p>이때,</p>
<ul class="simple">
<li><p>데이터셋에 있는 state-action pair <span class="math notranslate nohighlight">\((\mathbf{s},\mathbf{a})\in\mathcal{D}\)</span>의 empirical 분포와</p></li>
<li><p>학습된 정책 <span class="math notranslate nohighlight">\(\pi\)</span>가 만드는 state-action pair <span class="math notranslate nohighlight">\((\mathbf{s}, \mathbf{a})\)</span> where <span class="math notranslate nohighlight">\(\mathbf{s} \in \mathcal{D}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{a} \sim \pi(\cdot|\mathbf{s})\)</span>의 분포가</p></li>
</ul>
<p>다를 수 있으며, 이를 distributional shift라고 부른다. Online RL의 경우 학습된 정책으로 직접 환경과 상호작용하여 이전에 보지 못한 state-action pair을 경험해볼 수 있지만, offline RL의 경우 환경과 상호작용할 수 없기 때문에 학습된 정책으로 만든 state-action pair에 에러가 있을 경우 이를 correction할 수 없다.</p>
<br>
</section>
<section id="distribution-shift">
<h3>1.2 Distribution shift의 문제점 알아보기<a class="headerlink" href="#distribution-shift" title="Permalink to this headline">#</a></h3>
<p>Distribution shift가 Q-learning에 미치는 영향을 알아보기 위해 먼저 Bellman operator를 떠올려보자.</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\mathcal{B}^\pi Q^{\pi}(\mathbf{s, \mathbf{a}}) = r(\mathbf{s, \mathbf{a}}) + \gamma \mathbb{E}_{s' \sim p(\cdot|\mathbf{s}, \mathbf{a}), a' \in \pi(\cdot|\mathbf{s'})}\left[ Q^\pi(\mathbf{s'}, \mathbf{a'})\right].
\end{equation}
\]</div>
<br>
<p>일반적으로 reward function <span class="math notranslate nohighlight">\(r\)</span>과 transition probability distribution <span class="math notranslate nohighlight">\(p\)</span>를 알 수 없기 때문에, buffer <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>에 있는 transition <span class="math notranslate nohighlight">\((\mathbf{s}, \mathbf{a}, r, \mathbf{s}')\)</span>을 사용하여 행동가치함수를 학습한다. Empirical Bellman operator <span class="math notranslate nohighlight">\(\hat{\mathcal{B}}\)</span>는 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{B}}^\pi Q^{\pi}(\mathbf{s, \mathbf{a}}) = r + \gamma \mathbb{E}_{\mathbf{a}' \sim \hat{\pi}(\cdot | \mathbf{s}')}\left[ Q(\mathbf{s}', \mathbf{a}')\right] \; \text{ for } (\mathbf{s}, \mathbf{a}, r, \mathbf{s}') \in \mathcal{D}.
\]</div>
<br>
<p>보상 <span class="math notranslate nohighlight">\(r\)</span>과 다음 상태 <span class="math notranslate nohighlight">\(\mathbf{s}'\)</span>이 환경 모델로부터 샘플링되는 것이 아니라 그냥 데이터를 대입하는 것이다. Empirical Bellman operator를 바탕으로 TD 에러를 최소화하여 policy evaluation을 하는 것을 식으로 적어보면 다음과 같다.</p>
<div class="math notranslate nohighlight">
\[
\hat{Q}^{k+1} \leftarrow \operatorname*{argmin}_{Q}\mathbb{E}_{(\mathbf{s}, \mathbf{a}, r, \mathbf{s}') \in \mathcal{D}} \left[ \left( \left( r +\gamma\mathbb{E}_{\mathbf{a}' \sim \hat{\pi}^k(\cdot | \mathbf{s}')}\left[ \hat{Q}^k(\mathbf{s}', \mathbf{a}')\right]\right) - Q(\mathbf{s}, \mathbf{a}) \right)^2\right].
\]</div>
<br>
<p>위 식에서 <span class="math notranslate nohighlight">\((\mathbf{s}, \mathbf{a}, r, \mathbf{s}')\)</span>은 우리가 갖고 있는 데이터이다.</p>
<ul class="simple">
<li><p>문제가 되는 부분은 정책에서 샘플링한 다음 행동 <span class="math notranslate nohighlight">\(\mathbf{a}'\)</span>에 대해 계산되어야 하는 <span class="math notranslate nohighlight">\(\hat{Q}^k(\mathbf{s}', \mathbf{a}')\)</span>이다.</p></li>
<li><p>더 문제가 되는 경우는 우리의 데이터셋 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>에 <span class="math notranslate nohighlight">\((\mathbf{s}', \mathbf{a}')\)</span>에 대한 transition이 전혀 없는 경우이다. 이런 행동을 out-of-distribution action이라고 부른다.</p></li>
<li><p>이보다 더 문제가 되는 경우는 <span class="math notranslate nohighlight">\(\hat{Q}^k(\mathbf{s}', \mathbf{a}')\)</span>이 overestimation 되어 있는 경우이다. Offline RL의 경우 ood action에 대한 행동가치함수 <span class="math notranslate nohighlight">\(\hat{Q}^k(\mathbf{s}', \mathbf{a}')\)</span>는 타겟으로만 사용되고 직접 업데이트되지 않기 때문에 overestimation이 correction 될 수 없다.</p></li>
</ul>
<br>
<p>이 논문에서는 Conservative Q-Learning (CQL)이라는 알고리즘을 제시하여 overestimation을 근절시킨다. Conservative Q-learning으로 학습한 행동가치함수는 실제 행동가치함수의 lower bound가 된다.</p>
<br>
</section>
</section>
<hr class="docutils" />
<section id="conservative-q-learning">
<h2>2. Conservative Q-Learning<a class="headerlink" href="#conservative-q-learning" title="Permalink to this headline">#</a></h2>
<section id="conservative-off-policy-evaluation">
<span id="sec-policy-evaluation"></span><h3>2.1 Conservative Off-Policy Evaluation<a class="headerlink" href="#conservative-off-policy-evaluation" title="Permalink to this headline">#</a></h3>
<p>Function approximation을 사용하여 행동가치함수를 학습 중일 때, 그냥 단순하게 <span class="math notranslate nohighlight">\(Q(\mathbf{s}, \mathbf{a})\)</span> 값이 낮아지는 방향으로 업데이트를 하면 overestimation이 완화될 것이다. 따라서 다음과 같이 Q-network 업데이트 식에 <span class="math notranslate nohighlight">\(Q(\mathbf{s}, \mathbf{a})\)</span> 최소화하는 텀을 추가할 수 있을 것이다.</p>
<div class="math notranslate nohighlight">
\[
\hat{Q}^{k+1} \leftarrow \operatorname*{argmin}_{Q} \left(\textcolor{blue}{ \alpha \mathbb{E}_{\mathbf{s}\sim\mathcal{D}, \mathbf{a}\sim\mu(\cdot|\mathbf{s})}\left[ Q(\mathbf{s}, \mathbf{a}) \right]} + \frac{1}{2} \mathbb{E}_{\mathbf{s}, \mathbf{a} \sim \mathcal{D}} \left[ \left( Q(\mathbf{s}, \mathbf{a}) - \hat{\mathcal{B}}^{\pi}\hat{Q}^k(\mathbf{s}, \mathbf{a}) \right)^2\right] \right).
\]</div>
<br>
<p>기존의 TD 에러텀에 파란색으로 표시된 부분이 추가된 것 뿐이다. 여기서 <span class="math notranslate nohighlight">\(\mu(\cdot | \mathbf{s})\)</span>는 어떤 특정 분포이다. 만약 OOD 행동에 대해서만 Q값을 최소화하고 싶다면, 상태 <span class="math notranslate nohighlight">\(\mathbf{s}\)</span>에서 ood 행동에만 확률을 부여하는 확률분포 <span class="math notranslate nohighlight">\(\mu(\cdot | \mathbf{s})\)</span>를 구해서 사용하면 된다. 물론 그런 확률분포는 알기 어려울 것이다. 이후에 또 말하겠지만, <span class="math notranslate nohighlight">\(\mu(\mathbf{a}|\mathbf{s}) \propto \exp(Q(\mathbf{s}, \mathbf{a}))\)</span>되 게 설정하게 된다.</p>
<p>위 업데이트식으로 찾은 행동가치함수는 모든 <span class="math notranslate nohighlight">\((\mathbf{s}, \mathbf{a})\)</span>에 대해서 실제 행동가치함수의 lower bound가 된다는 것이 논문에 증명되어 있다 (Theorem 3.1).</p>
<br>
<p>그런데 위 업데이트식은 조금 억울하다. 그냥 <span class="math notranslate nohighlight">\(Q(\mathbf{s}, \mathbf{a})\)</span>의 크기를 막 줄여 버리는 느낌이다. Offline RL에서 더욱 문제가 되는 overestimation은 데이터셋에 없는 상태-행동 순서쌍에 대한 overestimation이다. 따라서, 데이터셋에 있는 상태-행동 순서쌍에 대한 행동가치함수는 다시 높여주는 텀을 추가해준다.</p>
<div class="math notranslate nohighlight" id="equation-policy-evaluation">
<span class="eqno">(1)<a class="headerlink" href="#equation-policy-evaluation" title="Permalink to this equation">#</a></span>\[\begin{split}
\hat{Q}^{k+1} \leftarrow \operatorname*{argmin}_{Q}  \alpha \left(\mathbb{E}_{\mathbf{s}\sim\mathcal{D}, \mathbf{a}\sim\mu(\cdot|\mathbf{s})}\left[ Q(\mathbf{s}, \mathbf{a}) \right]  \textcolor{blue}{- \mathbb{E}_{\mathbf{s}\sim\mathcal{D}, \mathbf{a}\sim \hat{\pi}_\beta(\cdot|\mathbf{s})} \left[  Q(\mathbf{s}, \mathbf{a})\right]} \right) \\ + \frac{1}{2} \mathbb{E}_{\mathbf{s}, \mathbf{a} \sim \mathcal{D}} \left[ \left( Q(\mathbf{s}, \mathbf{a}) - \hat{\mathcal{B}}^{\pi}\hat{Q}^k(\mathbf{s}, \mathbf{a}) \right)^2\right],
\end{split}\]</div>
<br>
<p>여기서 <span class="math notranslate nohighlight">\(\hat{\pi}_\beta(\mathbf{a}|\mathbf{s})\)</span>는 데이터셋에서 빈도를 세서 만든 empirical 정책이다. 데이터셋 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>에서 <span class="math notranslate nohighlight">\((\mathbf{s}, \mathbf{a})\)</span>가 등장한 횟수를 상태 <span class="math notranslate nohighlight">\(\mathbf{s}\)</span>가 등장한 횟수로 나눠서 확률을 부여하는 확률분포이다. <span class="math notranslate nohighlight">\(\mu(\mathbf{a}|\mathbf{s})\)</span>를 따라서 행동가치함수 값을 낮춰주되 <span class="math notranslate nohighlight">\(\hat{\pi}_\beta(\mathbf{a}|\mathbf{s})\)</span>을 따라서 행동가치함수를 올려주는 방식으로 행동가치함수를 찾게 된다.</p>
<p>위 업데이트식을 사용하여 찾은 행동가치함수을 경우, 행동가치함수의 기댓값인 상태가치함수가 실제 상태가치함수의 lower bound가 되게 된다. 그리고 실제 상태가치함수와 더 가까운 tighter한 lower bound가 된다 (Theorem 3.2).</p>
<br>
</section>
<section id="conservative-q-learning-for-offline-rl">
<h3>2.2 Conservative Q-Learning for Offline RL<a class="headerlink" href="#conservative-q-learning-for-offline-rl" title="Permalink to this headline">#</a></h3>
<p>사실 <a class="reference internal" href="#sec-policy-evaluation"><span class="std std-ref">2.1 Conservative Off-Policy Evaluation</span></a>의 내용들은 행동 정책이 만든 데이터셋으로부터 어떤 타겟 정책의 행동가치함수를 추정하는 off-policy evaluation 방법이기 때문에 굳이 offline RL에 국한된 이야기는 아니다. 저자는 위 policy evaluation 방법을 off-policy learning에 사용해도 된다고 말한다.</p>
<p>그러면 어떻게 위의 policy evaluation을 policy optimization에 이용할 수 있을까? 일반적인 policy iteration은 다음 두 과정을 반복한다.</p>
<ul class="simple">
<li><p>Policy evaluation: 현재 정책 <span class="math notranslate nohighlight">\(\pi_k\)</span>의 행동가치함수 <span class="math notranslate nohighlight">\(Q^{\pi_k}\)</span>를 추정</p></li>
<li><p>Policy improvement: 현재 행동가치함수 <span class="math notranslate nohighlight">\(Q^{\pi_k}\)</span>를 증가시키는 방향으로 정책 업데이트</p></li>
</ul>
<p>식 <a class="reference internal" href="#equation-policy-evaluation">(1)</a>의 최적화 문제를 풀어서 행동가치함수를 구하고, 또 다시 이를 최대화하는 정책을 찾는 방식으로 정책을 업데이트하면 시간이 많이 걸릴 것이다.  한편, Actor-critic 알고리즘에서는 Q-network를 최대화하는 방향으로 정책이 업데이트된다. 이와 유사하게 현재 행동가치함수를 최대화하는 <span class="math notranslate nohighlight">\(\mu(\mathbf{a}|\mathbf{s})\)</span>를 찾아주는 텀을 추가하여 정책과 행동가치함수를 동시에 최적화하는 문제를 생각해볼 수 있다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\min_{Q} \textcolor{blue}{\max_{\mu}} \alpha \left(\mathbb{E}_{\mathbf{s}\sim\mathcal{D}, \textcolor{blue}{\mathbf{a}\sim\mu(\cdot|\mathbf{s})}}\left[ Q(\mathbf{s}, \mathbf{a}) \right]  - \mathbb{E}_{\mathbf{s}\sim\mathcal{D}, \mathbf{a}\sim \hat{\pi}_\beta(\cdot|\mathbf{s})} \left[  Q(\mathbf{s}, \mathbf{a})\right] \right) \\ + \frac{1}{2} \mathbb{E}_{\mathbf{s}, \mathbf{a} \sim \mathcal{D}} \left[ \left( Q(\mathbf{s}, \mathbf{a}) - \hat{\mathcal{B}}^{\pi}\hat{Q}^k(\mathbf{s}, \mathbf{a}) \right)^2\right] + \textcolor{blue}{\mathcal{R}(\mu)},
\end{split}\]</div>
<br>
<p>여기서, <span class="math notranslate nohighlight">\(\mathcal{R}(\mu)\)</span>는 정책에 대한 regularizer이다. 어떤 regularizer를 사용하느냐에 따라 최적화 문제가 달라지기 때문에 위의 최적화 식을 <span class="math notranslate nohighlight">\(\text{CQL}(\mathcal{R})\)</span>이라고 명명하였다. 보통 강화학습에서는 정책 <span class="math notranslate nohighlight">\(\mu\)</span>이 너무 확확 업데이트되는 것을 싫어한다. 그래서 주로 특정 분포 <span class="math notranslate nohighlight">\(\rho\)</span>와 너무 멀어지지 않도록 제약을 주고 정책을 업데이트한다. 제약은 주로 KL divergence로 주기 때문에 <span class="math notranslate nohighlight">\(\mathcal{R}(\mu) = -D_{\text{KL}}(\mu || \rho)\)</span>로 준다. 위의 제약을 사용하면 <span class="math notranslate nohighlight">\(\mu(\mathbf{a}|\mathbf{s}) \propto \rho(\mathbf{a}|\mathbf{s}) \cdot \exp(Q(\mathbf{s}, \mathbf{a}))\)</span> 꼴이어야만 한다.</p>
<br>
<p>많은 알고리즘에서 <span class="math notranslate nohighlight">\(\rho\)</span>를 이전 정책 <span class="math notranslate nohighlight">\(\hat{\pi}^k\)</span>로 준다. 이 논문에서는 <span class="math notranslate nohighlight">\(\rho\)</span>로 uniform 분포를 주는 경우를 <span class="math notranslate nohighlight">\(\text{CQL}(\mathcal{\mathcal{H}})\)</span>으로 부른다. 어떤 분포와 균등 분포 사이의 KL divergence를 계산하는 것이 해당 분포의 엔트로피를 계산하는 것과 상수배 차이이기 때문이다.</p>
<div class="math notranslate nohighlight" id="equation-cqlh">
<span class="eqno">(2)<a class="headerlink" href="#equation-cqlh" title="Permalink to this equation">#</a></span>\[\begin{split}
\min_{Q} \alpha \mathbb{E}_{\mathbf{s} \in \mathcal{D}} \left[ \log \sum_{\mathbf{a}} \exp(Q(\mathbf{s}, \mathbf{a})) - \mathbb{E}_{\mathbf{a} \sim \hat{\pi}_{\beta}(\cdot|\mathbf{s})}\left[ Q(\mathbf{s}, \mathbf{a}) \right] \right] \\ + \frac{1}{2} \mathbb{E}_{\mathbf{s}, \mathbf{a}, \mathbf{s}' \sim \mathcal{D}} \left[ \left( Q(\mathbf{s}, \mathbf{a}) - \hat{\mathcal{B}}^{\pi_k}\hat{Q}^k(\mathbf{s}, \mathbf{a}) \right)^2 \right].
\end{split}\]</div>
<br>
<p>위 식이 CQL에서 사용하는 Q-network의 업데이트식이다. 정책 <span class="math notranslate nohighlight">\(\mu\)</span> 최대화 텀이 어디 갔는지 궁금할 수도 있다. 사실, <span class="math notranslate nohighlight">\(\max\limits_\mu \left( \mathbb{E}[Q(\mathbf{s}, \mathbf{a})] -D_{\text{KL}}(\mu || \rho) \right)\)</span>을 직접 해석적으로 풀어보면 <span class="math notranslate nohighlight">\(\log \sum_{\mathbf{a}} \exp(Q(\mathbf{s}, \mathbf{a}))\)</span>이 된다. 정책 업데이트 이야기는 식을 유도하기 위함이었고, 사실 여기에는 정책 네트워크가 낄 자리는 없다. 그래서 Q-network를 업데이트하기 위해서만 사용된다.</p>
<br>
<p>최종적으로 <span class="math notranslate nohighlight">\(\text{CQL}(\mathcal{\mathcal{H}})\)</span>의 수도 코드를 보면 다음과 같다. 수도코드에 있는 Equation 4가 본 글에서의 식 <a class="reference internal" href="#equation-cqlh">(2)</a>이다.</p>
<figure class="align-default" id="cql-algorithm">
<a class="reference internal image-reference" href="../../_images/230530_cql_algorithm.png"><img alt="../../_images/230530_cql_algorithm.png" src="../../_images/230530_cql_algorithm.png" style="width: 500px;" /></a>
</figure>
<br>
</section>
</section>
<hr class="docutils" />
<section id="experimental-evaluation">
<h2>Experimental Evaluation<a class="headerlink" href="#experimental-evaluation" title="Permalink to this headline">#</a></h2>
<p>D4RL의 mujoco 도메인에 대한 결과는 아래 표와 같다.</p>
<figure class="align-default" id="cql-result-mujoco">
<img alt="../../_images/230530_cql_result_mujoco.png" src="../../_images/230530_cql_result_mujoco.png" />
</figure>
<br>
</section>
<hr class="docutils" />
<section id="id2">
<h2>마무리하며<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>Offline RL 쪽을 연구하고 싶어서 가장 유명한 논문부터 읽어보았다. 수식이 너무 많아서 처음에는 하나도 이해가 안 되었는데, 반복해서 읽다보니 흐름은 잘 이해가 되었다. 본 논문의 꽃은 사실 이론들인데, 아직 이론까지는 정독하지 못해서 본 글에서는 다루지 않았다. 이론 쪽도 읽어 볼 예정이기 때문에 기회가 된다면 이론과 증명을 추가하도록 할 것이다. 빠른 시일 내로 offline RL 실험 환경 구축에 관한 글을 작성할 예정이다.</p>
<br>
</section>
<hr class="docutils" />
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">#</a></h2>
<p>[1] Kumar, Aviral, Aurick Zhou, George Tucker, and Sergey Levine. “Conservative Q-Learning for Offline Reinforcement Learning.” In Advances in Neural Information Processing Systems, 33:1179–91. Curran Associates, Inc., 2020. <a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html</a>.</p>
<br>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./contents/rl_papers"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="DreamerV1.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Dreamer</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../miscellaneous/define_and_by_run.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Define-and-run vs Define-by-run</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 재야의 숨은 초보<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>